<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zhangyang Gao, 高张阳">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/icon.PNG">
<title>Zhangyang Gao</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#2a7ce0"><img src="./Files/gaozhangyang_photo.jpg" alt="" height="285px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#2a7ce0"><font size="4">Zhangyang Gao (</font><font size="4"; font style="font-family:Microsoft YaHei">高张阳</font><font size="4">)</font></a><br />
<i>Ph.D Candidate, <a href="https://www.zju.edu.cn/" target="_blank" style="color:#2a7ce0">Zhejiang University</a> & <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a></i>
<br /><br />
<a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Center for Artificial Intelligence Research and Innovation (CAIRI)</a><br />
<a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a><br />
<br />
Location: E2-223, Westlake University, Dunyu Road #600, Hangzhou, Zhejiang, China<br />
<class="staffshortcut">
 <A HREF="#News" style="color:#2a7ce0">News</A> | 
 <A HREF="#Interest" style="color:#2a7ce0">Research Interest</A> | 
 <A HREF="#Education" style="color:#2a7ce0">Education</A> | 
 <A HREF="#Publications" style="color:#2a7ce0">Publications</A> | 
 <A HREF="#Services" style="color:#2a7ce0">Services</A> | 
 <A HREF="#Awards" style="color:#2a7ce0">Awards</A>
<br />
<br />
 
Email: gaozhangyang@westlake.edu.cn <br />
[<a href="https://scholar.google.com/citations?user=4SclT-QAAAAJ&hl=zh-CN&oi=ao" target="_blank" style="color:#2a7ce0">Google Scholar</a>] 
[<a href="https://github.com/gaozhangyang" target="_blank" style="color:#2a7ce0">GitHub</a>] 
[<a href="https://www.researchgate.net/profile/Gao-Zhangyang" target="_blank" style="color:#2a7ce0">ResearchGate</a>] 
[<a href="https://orcid.org/0000-0003-1026-6083" target="_blank" style="color:#2a7ce0">ORCID</a>] 

<br />
<br />
<i>Welcome to contacting me about research or internship by emails or WeChat (simpler_yet_better).</i><br />
<i></i>I am eagerly seeking a postdoctoral position beginning in August 2025!</i><br />
</td></tr></table>


<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:300px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#FF0000">[2024.09]</font> </b> One paper on, <i><font color="#2a7ce0">graph contrastive learning</font></i>, has been accepted by <b>TNNLS</b>. </li>
    <li><b> <font color="#FF0000">[2024.07]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>CIKM 2024</b>. </li>
    <li><b> <font color="#FF0000">[2024.05]</font> </b> One paper on, <i><font color="#2a7ce0">AI4Science</font></i>, has been accepted by <b>ICML 2024</b>. </li>
    <li><b> <font color="#FF0000">[2024.05]</font> </b> Four co-authored papers on, <i><font color="#2a7ce0">AI4Science</font></i>, have been accepted by <b>ICML 2024</b>. </li>
    <li><b> <font color="#FF0000">[2024.03]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>TKDE</b>. </li>
    <li><b> <font color="#FF0000">[2024.01]</font> </b> One paper on, <i><font color="#2a7ce0">AI4Science</font></i>, has been accepted by <b>ICLR 2024 (<font color="#FF0000">Spotlight</font>)</b>. </li>
    <li><b> <font color="#FF0000">[2024.01]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph SSL</font></i>, has been accepted by <b>ICLR 2024</b>. </li>
    <li><b> <font color="#FF0000">[2023.12]</font> </b> One paper on, <i><font color="#2a7ce0">protein pre-training</font></i>, has been accepted by <b>AAAI 2024</b>. </li>
    <li><b> <font color="#FF0000">[2023.12]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">AI4Science</font></i>, have been accepted by <b>AAAI 2024</b>. </li>
    <li><b> <font color="#FF0000">[2023.09]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">AI4Science</font></i>, have been accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.09]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">mixup</font></i> and <i><font color="#2a7ce0">video prediction</font></i>, have been accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.08]</font> </b> One paper on, <i><font color="#2a7ce0">protein modeling</font></i>, has been accepted by <b>Communications Biology</b>, congrats to <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Fang Wu</a>. </li>
    <li><b> <font color="#FF0000">[2023.06]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>TKDE</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2023.06]</font> </b> One paper on, <i><font color="#2a7ce0">graph augmentation</font></i>, has been accepted by <b>ECML 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.04]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>ICML 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.03]</font> </b> One paper on, <i><font color="#2a7ce0">graph structure learning</font></i>, has been accepted by <b>TNNLS</b>. </li>
    <!-- <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph embedding</font></i>, has been accepted by <b>ICASSP 2023</b>, congrats to <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=6FZh9C8AAAAJ=en" target="_blank" style="color:#2a7ce0">Bozhen Hu</a>. </li> -->
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">spatiotemporal prediction</font></i>, has been accepted by <b>CVPR 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#FF0000">[2022.12]</font> </b> One paper on, <i><font color="#2a7ce0">heterogeneous graph</font></i>, has been accepted by <b>TNNLS</b>. </li>
    
    <li><b> <font color="#FF0000">[2022.11]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>AAAI 2023 (<font color="#FF0000">Oral</font>)</b>. </li>
    <li><b> <font color="#FF0000">[2022.09]</font> </b> Two papers on, <i><font color="#2a7ce0">graph augmentation and attack</font></i>, have been accepted by <b>NeurIPS 2022 (<font color="#FF0000">Spotlight</font>)</b>. </li>
    <li><b> <font color="#FF0000">[2022.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph attack</font></i>, has been accepted by <b>CIKM 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=OgIdbfAAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Zihan Liu</a>. </li>
    <li><b> <font color="#FF0000">[2022.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">temporal point process</font></i>, has been accepted by <b>TMLR</b>, congrats to <a href="https://scholar.google.com/citations?user=o5A23qIAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Haitao Lin</a>. </li>
    <li><b> <font color="#FF0000">[2022.07]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">mixup augmentation</font></i>, has been accepted by <b>ECCV 2022 (<font color="#FF0000">Oral</font>)</b>, congrats to <a href="https://scholar.google.com/citations?user=EwMGZsgAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Zicheng Liu</a>. </li>
    <li><b> <font color="#FF0000">[2022.06]</font> </b> One paper on, <i><font color="#2a7ce0">class-imbalanced classification</font></i>, has been accepted by <b>ECML 2022 (<font color="#FF0000">Oral</font>)</b>. </li>
    
    <li><b> <font color="#FF0000">[2022.05]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph contrastive learning</font></i>, has been accepted by <b>ICML 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2022.03]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">video prediction</font></i>, has been accepted by <b>CVPR 2022</b>, congrats to <a href="https://scholar.google.com/citations?hl=en&user=4SclT-QAAAAJ" target="_blank" style="color:#2a7ce0">Zhangyang Gao</a>. </li>
    <li><b> <font color="#FF0000">[2022.03]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">semi-supervised learning</font></i>, has been accepted by <b>CVPR 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <!-- <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>ICASSP 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li> -->
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One paper on, <i><font color="#2a7ce0">deep clustering</font></i>, has been accepted by <b>TNNLS</b>. </li>
    <li><b> <font color="#FF0000">[2022.01]</font> </b> One paper on, <i><font color="#2a7ce0">graph contrastive learning</font></i>, has been accepted by <b>WWW 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <!-- <li><b> <font color="#FF0000">[2022.01]</font> </b> One paper on, <i><font color="#2a7ce0">disentanglement learning</font></i>, has been accepted by <b>NCAA</b>. </li> -->

    <li><b> <font color="#FF0000">[2021.12]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">spatio-temporal forecasting</font></i>, has been accepted by <b>AAAI 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=o5A23qIAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Haitao Lin</a>. </li>
    <li><b> <font color="#FF0000">[2021.11]</font> </b> One paper on, <i><font color="#2a7ce0">graph self-supervised learning</font></i>, has been accepted by <b>TKDE</b>. </li>
    <li><b> <font color="#FF0000">[2021.10]</font> </b> One paper on, <i><font color="#2a7ce0">deep clustering</font></i>, has been accepted by <b>WACV 2022</b>. </li>
    <li><b> <font color="#FF0000">[2021.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>ACM MM 2021 (<font color="#FF0000">Oral</font>)</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#FF0000">[2021.07]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">invertible learning</font></i>, has been accepted by <b>ECML 2021</b>, congrats to <a href="https://scholar.google.com/citations?user=SKTQTXwAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Siyuan Li</a>. </li>
    <li><b> <font color="#FF0000">[2020.10]</font> </b> One paper on, <i><font color="#2a7ce0">mass spectrometry</font></i>, has been accepted by <b>JASMS</b>. </li>
    <li><b> <font color="#FF0000">[2020.09]</font> </b> One paper on, <i><font color="#2a7ce0">video compression</font></i>, has been accepted by <b>TCSVT</b>. </li>
    <li><b> <font color="#FF0000">[2020.09]</font> </b> Got my B.E. degree! </li>
    <li><b> <font color="#FF0000">[2019.10]</font> </b> One paper on, <i><font color="#2a7ce0">image compression</font></i>, has been accepted by <b>WACV 2020</b>. </li>
  </ul>
</div>



<script>
	$(function(){
		$(".t").click(function(){
			var children = $(this).siblings(".txt")
			if(children.is(":hidden")){
				children.show();
			}
		})
	})
</script>



 
<A NAME="Interest"><h1>Research Interest</h1></A>
Currently, I focus on the following research topics:
<ul>
<li>Video Prediction</li>
<li>AI4Science (Molecule Modeling)</li>
</ul>
<br />



 
<A NAME="Education"><h1>Education</h1></A>
<ul>
<li>2020.09-present &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D in <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">CAIRI</a>, <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#2a7ce0">Stan Z. Li</a></li>
<li>2016.09-2020.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.E. in <a href="https://soa.csu.edu.cn/" target="_blank" style="color:#2a7ce0">School of Automation</a>, <a href="https://www.csu.edu.cn/" target="_blank" style="color:#2a7ce0">Central South University</a>.</li>
</ul>
<br />

 


<A NAME="Publications"><h1>Publications</h1></A>

<!-- <p><b><font color="#000000" size=4.5> Selected: </font></b></p>
<font size="3"> 
<ul>
    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_NIPS_ProteinInvBench_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=zxxSJAVQPc" target="_blank" style="color:#2a7ce0">Proteininvbench: Benchmarking protein inverse folding on diverse tasks, models, and metrics</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Yijie Zhang, Xingran Chen, Lirong Wu, Stan Z.Li </a></i><br><i><b>NeurIPS, 2023</b></i><br>
            [<a href= "https://proceedings.neurips.cc/paper_files/paper/2023/file/d73078d49799693792fb0f3f32c57fc8-Paper-Datasets_and_Benchmarks.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/OpenCPD" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_NIPS_ProteinInvBench_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2024_ICML_GraphsGPT_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=zxxSJAVQPc" target="_blank" style="color:#2a7ce0">A Graph is Worth $ K $ Words: Euclideanizing Graph using Pure Transformer</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z.Li </a></i><br><i><b>ICML, 2024</b></i><br>
            [<a href= "https://openreview.net/pdf?id=zxxSJAVQPc" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/GraphsGPT" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_ICML_GraphsGPT_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2024_ICLR_KWDesign_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=mpqMVWgqjn" target="_blank" style="color:#2a7ce0">KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, Stan Z.Li </a></i><br><i><b>ICLR, 2024</b></i><br>
            [<a href= "https://openreview.net/pdf?id=mpqMVWgqjn" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/ProteinInvBench" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_ICLR_KWDesign_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2023_ICLR_PiFold_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=oMsN9TYwJ0j" target="_blank" style="color:#2a7ce0">PiFold: Toward effective and efficient protein inverse folding</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Stan Z.Li </a></i><br><i><b>ICLR, 2023 (Spotlight)</b></i><br>
            [<a href= "https://openreview.net/pdf?id=oMsN9TYwJ0j" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/PiFold" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_ICLR_PiFold_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2022_CVPR_SimVP_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_SimVP_Simpler_Yet_Better_Video_Prediction_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">Simvp: Simpler yet better video prediction</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Lirong Wu, Stan Z.Li </a></i><br><i><b>CVPR, 2022</b></i><br>
            [<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_SimVP_Simpler_Yet_Better_Video_Prediction_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/SimVP" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2022_CVPR_SimVP_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_Arxiv_FoldToken_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2403.09673" target="_blank" style="color:#2a7ce0">Foldtoken: Learning protein language via vector quantization and beyond</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z.Li </a></i><br><i><b>Arxiv, 2024</b></i><br>
            [<a href= "https://arxiv.org/pdf/2403.09673" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_Arxiv_FoldToken_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_Arxiv_FoldToken2_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://www.biorxiv.org/content/biorxiv/early/2024/06/13/2024.06.11.598584.1.full.pdf" target="_blank" style="color:#2a7ce0">FoldToken2: Learning compact, invariant and generative protein structure language</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Stan Z.Li </a></i><br><i><b>Arxiv, 2024</b></i><br>
            [<a href= "https://www.biorxiv.org/content/biorxiv/early/2024/06/13/2024.06.11.598584.1.full.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://colab.research.google.com/drive/1_z7qy4Vpomy7kzn1oxbjVHUTik47HSSX?usp=sharing" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_Arxiv_FoldToken2_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_Arxiv_FoldToken3_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://www.biorxiv.org/content/10.1101/2024.07.08.602548v1.full.pdf" target="_blank" style="color:#2a7ce0">FoldToken3: Fold Structures Worth 256 Words or Less</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Stan Z.Li </a></i><br><i><b>Arxiv, 2024</b></i><br>
            [<a href= "https://www.biorxiv.org/content/10.1101/2024.07.08.602548v1.full.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_Arxiv_FoldToken3_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_Arxiv_FoldToken4_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://www.biorxiv.org/content/biorxiv/early/2024/08/04/2024.08.04.606514.1.full.pdf" target="_blank" style="color:#2a7ce0">FoldToken4: Consistent & Hierarchical Fold Language</a></b></font><br>
            <i> <b>Zhangyang Gao</b>, Cheng Tan, Stan Z.Li </a></i><br><i><b>Arxiv, 2024</b></i><br>
            [<a href= "https://www.biorxiv.org/content/biorxiv/early/2024/08/04/2024.08.04.606514.1.full.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_Arxiv_FoldToken4_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>
</ul>
</font>
<br />
<br />
<br />

<p><b><font color="#000000" size=4.5> (Co-) First Author: </font></b></p>
<font size="3"> 
<ul>

    

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_CVPR_GPM_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_General_Point_Model_Pretraining_with_Autoencoding_and_Autoregressive_CVPR_2024_paper.pdf" target="_blank" style="color:#2a7ce0">General Point Model Pretraining with Autoencoding and Autoregressive</a></b></font><br>
            <i> Zhe Li, <b>Zhangyang Gao*</b>, Cheng Tan,  Bocheng Ren, Laurence T Yang, Stan Z.</a></i><br><i><b>CVPR, 2024</b></i><br>
            [<a href= "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_General_Point_Model_Pretraining_with_Autoencoding_and_Autoregressive_CVPR_2024_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_CVPR_GPM_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_AAAI_ADesigner_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ojs.aaai.org/index.php/AAAI/article/view/29445" target="_blank" style="color:#2a7ce0">Cross-Gate MLP with Protein Complex Invariant Embedding Is a One-Shot Antibody Designer</a></b></font><br>
            <i> Cheng Tan, <b>Zhangyang Gao*</b>, Lirong Wu,  Jun Xia, Jiangbin Zheng, Xihong Yang, Yue Liu, Bozhen Hu, Stan Z.</a></i><br><i><b>AAAI, 2024</b></i><br>
            [<a href= "https://ojs.aaai.org/index.php/AAAI/article/view/29445" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/A4Bio/ADesigner" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_AAAI_ADesigner_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_ECML_CoSP_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://link.springer.com/chapter/10.1007/978-3-031-43412-9_24" target="_blank" style="color:#2a7ce0">Co-supervised Pre-training of Pocket and Ligand</a></b></font><br>
            <i> Zhangyang Gao, Cheng Tan, Jun Xia, Stan Z.</a></i><br><i><b>ECML, 2023</b></i><br>
            [<a href= "https://link.springer.com/chapter/10.1007/978-3-031-43412-9_24" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2023_ECML_CoSP_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_ECML_SiamFlow_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://link.springer.com/chapter/10.1007/978-3-031-43427-3_25" target="_blank" style="color:#2a7ce0">Target-aware molecular graph generation</a></b></font><br>
            <i> Cheng Tan, <b>Zhangyang Gao*</b>, Stan Z.</a></i><br><i><b>ECML, 2023</b></i><br>
            [<a href= "https://link.springer.com/chapter/10.1007/978-3-031-43427-3_25" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2023_ECML_SiamFlow_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_ICASSP_GCA_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/document/10095229" target="_blank" style="color:#2a7ce0">Global-context aware generative protein design</a></b></font><br>
            <i> Cheng Tan, <b>Zhangyang Gao*</b>, Jun Xia,  Bozhen Hu, Stan Z.</a></i><br><i><b>ICASSP, 2023</b></i><br>
            [<a href= "https://ieeexplore.ieee.org/document/10095229" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/chengtan9907/Global-context-aware-generative-protein-design" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_ICASSP_GCA_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>
    
    <table class="imgtable"><tr><td>
        <img src="./Files/2023_CVPR_TAU_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.pdf" target="_blank" style="color:#2a7ce0">Temporal attention unit: Towards efficient spatiotemporal predictive learning</a></b></font><br>
            <i> Cheng Tan, <b>Zhangyang Gao*</b>, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li and Li, Stan Z.</a></i><br><i><b>CVPR, 2023</b></i><br>
            [<a href= "https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_CVPR_TAU_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2022_AAAI_CLCRN_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ojs.aaai.org/index.php/AAAI/article/view/20711" target="_blank" style="color:#2a7ce0">Conditional local convolution for spatio-temporal meteorological forecasting</a></b></font><br>
            <i> Haitao Lin, <b>Zhangyang Gao*</b>, Yongjie Xu, Ling Li, Stan Z.Li </a></i><br><i><b>AAAI, 2021</b></i><br>
            [<a href= "https://ojs.aaai.org/index.php/AAAI/article/view/20711/20470" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/EDAPINENUT/CLCRN" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2022_AAAI_CLCRN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2022_CVPR_HCR_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Tan_Hyperspherical_Consistency_Regularization_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">Hyperspherical consistency regularization</a></b></font><br>
            <i> Cheng Tan, <b>Zhangyang Gao*</b>, Lirong Wu, Siyuan Li, Stan Z.Li </a></i><br><i><b>CVPR, 2022</b></i><br>
            [<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Tan_Hyperspherical_Consistency_Regularization_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/chengtan9907/Hyperspherical-Consistency-Regularization" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2022_CVPR_HCR_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

</ul>
</font>
<br />
<br />
<br />

 
 
<p><b><font color="#000000" size=4.5> Co-Authors: </font></b></p>
<font size="3"> 
<ul>
    

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_ICDE_DiscoGNN_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10597976" target="_blank" style="color:#2a7ce0">DiscoGNN: A Sample-Efficient Framework for Self-Supervised Graph Representation Learning</a></b></font><br>
            <i> Jun Xia, Shaorong Chen, Yue Liu, Zhangyang Gao, Jiangbin Zheng,  Xihong Yang, Stan.Z</a></i>
            <br><i><b>ICDE, 2024 </b></i><br>
            [<a href= "https://ieeexplore.ieee.org/abstract/document/10597976" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_ICDE_DiscoGNN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_AAAI_PSCCPI_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ojs.aaai.org/index.php/AAAI/article/view/27784" target="_blank" style="color:#2a7ce0">PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction</a></b></font><br>
            <i> Lirong Wu, Yufei Huang, Chen Tan, Zhangyang Gao, Bozhen Hu, Haitao Lin, Zicheng Liu, Stan.Z</a></i>
            <br><i><b>AAAI, 2024 </b></i><br>
            [<a href= "https://ojs.aaai.org/index.php/AAAI/article/view/27784" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/LirongWu/PSC-CPI" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_AAAI_PSCCPI_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_NueralComputing_COCOMMR_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://link.springer.com/article/10.1007/s00521-024-10310-2" target="_blank" style="color:#2a7ce0">Enhancing human-like multimodal reasoning: a new challenging dataset and comprehensive framework</a></b></font><br>
            <i> Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Bihui Yu, Ruifeng Guo & Stan.Z</a></i>
            <br><i><b>Neural Computing and Applications, 2024 </b></i><br>
            [<a href= "https://link.springer.com/article/10.1007/s00521-024-10310-2" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_NueralComputing_COCOMMR_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_AAAI_SAO_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ojs.aaai.org/index.php/AAAI/article/view/29161" target="_blank" style="color:#2a7ce0">Protein 3D Graph Structure Learning for Robust Structure-Based Protein Property Prediction</a></b></font><br>
            <i> Yufei Huang, Siyuan Li, Lirong Wu, Jin Su, Haitao Lin, Odin Zhang, Zihan Liu, Zhangyang Gao, Jiangbin Zheng, Stan.Z</a></i>
            <br><i><b>AAAI, 2024 </b></i><br>
            [<a href= "https://ojs.aaai.org/index.php/AAAI/article/view/29161" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_AAAI_SAO_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>


    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_TNNLS_TGS_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10476767" target="_blank" style="color:#2a7ce0">A Teacher-Free Graph Knowledge Distillation Framework With Dual Self-Distillation </a></b></font><br>
            <i> Lirong Wu, Haitao Lin, Zhangyang Gao, Guojiang Zhao, Stan.Z</a></i>
            <br><i><b>TNNLS, 2024 </b></i><br>
            [<a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10476767" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/LirongWu/TGS" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_TNNLS_TGS_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>


    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_ICML_ReDock_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=QRjTDhCIO8" target="_blank" style="color:#2a7ce0">Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge  </a></b></font><br>
            <i> Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan.Z</a></i>
            <br><i><b>ICML, 2024 (Spotlight)</b></i><br>
            [<a href= "https://openreview.net/pdf?id=QRjTDhCIO8" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_ICML_ReDock_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>


    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_CVPR_MLIP_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MLIP_Enhancing_Medical_Visual_Representation_with_Divergence_Encoder_and_Knowledge-guided_CVPR_2024_paper.pdf" target="_blank" style="color:#2a7ce0">Mlip: Enhancing medical visual representation with divergence encoder and knowledge-guided contrastive learning </a></b></font><br>
            <i> Zhe Li, Laurence T. Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, Stan Z.</a></i><br><i><b>CVPR, 2024</b></i><br>
            [<a href= "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MLIP_Enhancing_Medical_Visual_Representation_with_Divergence_Encoder_and_Knowledge-guided_CVPR_2024_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="./Files/2024_CVPR_MLIP_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2024_TKDE_Diffusion_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10419041" target="_blank" style="color:#2a7ce0">A Survey on Generative Diffusion Models  </a></b></font><br>
            <i> Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, Stan Z.</a></i><br><i><b>TKDE, 2024</b></i><br>
            [<a href= "https://ieeexplore.ieee.org/abstract/document/10419041" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2024_TKDE_Diffusion_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>


    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_NIPS_OpenSTL_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://proceedings.neurips.cc/paper_files/paper/2023/file/dcbff44d11130e75d09d3930411c23e1-Paper-Datasets_and_Benchmarks.pdf" target="_blank" style="color:#2a7ce0">Openstl: A comprehensive benchmark of spatio-temporal predictive learning</a></b></font><br>
            <i>  Cheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu, Stan Z.Li</a></i><br><i><b>NeurIPS, 2023</b></i><br>
            [<a href= "https://proceedings.neurips.cc/paper_files/paper/2023/file/dcbff44d11130e75d09d3930411c23e1-Paper-Datasets_and_Benchmarks.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github. com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_NIPS_OpenSTL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_ICLR_MoleBert_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/pdf?id=jevY-DtiZTR" target="_blank" style="color:#2a7ce0">Mole-bert: Rethinking pre-training graph neural networks for molecules</a></b></font><br>
            <i> Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z.Li</a></i><br><i><b>ICLR, 2023</b></i><br>
            [<a href= "https://openreview.net/pdf?id=jevY-DtiZTR" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/junxia97/Mole-BERT" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_ICLR_MoleBert_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td></td>
        <img src="./Files/2023_TNNLS_RFAGNN_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10011169" target="_blank" style="color:#2a7ce0">Beyond homophily and homogeneity assumption: Relation-based frequency adaptive graph neural networks</a></b></font><br>
            <i> Lirong Wu, Haitao Lin, Bozhen Hu, Cheng Tan, <b>Zhangyang Gao</b>, Zicheng Liu, Stan Z.Li </a></i><br><i><b>TNNLS, 2023</b></i><br>
            [<a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10011169" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/LirongWu/RFA-GNN" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2023_TNNLS_RFAGNN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>

    <table class="imgtable"><tr><td>
        <img src="./Files/2021_TKDE_GraphSSL_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
        <td align="left"><p>
            <font size="2pt" face="Georgia"><b><a href= "https://dl.acm.org/doi/10.1109/TKDE.2021.3131584" target="_blank" style="color:#2a7ce0">Self-supervised learning on graphs: Contrastive, generative, or predictive</a></b></font><br>
            <i> Lirong Wu, Haitao Lin, Cheng Tan, <b>Zhangyang Gao</b>, Stan Z.Li </a></i><br><i><b>TKDE, 2021</b></i><br>
            [<a href= "https://arxiv.org/pdf/2105.07342" target="_blank" style="color:#2a7ce0">PDF</a>] 
            [<a href="https://github.com/LirongWu/awesome-graph-self-supervised-learning" target="_blank" style="color:#2a7ce0">Github</a>] 
            [<a href="./Files/2021_TKDE_GraphSSL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
    </p></td></tr></table>


</ul>
<br />
  -->


 
<A NAME="Services"><h1>Services</h1></A>
 
 
<p><b><font color="#000000" size=4.5> Program committee member | Reviewer </font></b></p>
<font size="3"> 
<ul>
    <li>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</li>
    <li>International Conference on Computer Vision (<b>ICCV</b>)</li>
    <li>International Conference on Learning Representations (<b>ICLR</b>)</li>
    <li>Conference and Workshop on Neural Information Processing Systems (<b>NeurIPS</b>)</li>
    <li>International Conference on Machine Learning (<b>ICML</b>)</li>
</ul>
</font>
<br />

  </div>


</body>
</html>
